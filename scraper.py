#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import re
import hashlib

import cloudscraper
from bs4 import BeautifulSoup
import cv2

# ==== é…ç½® ====
BRIGHTNESS_THRESHOLD = 130
BATCH_SIZE = 100
IMAGES_DIR = "ri"
GALLERIES_FILE = "galleries.json"
PROGRESS_FILE = "progress.json"
COUNT_FILE = os.path.join(IMAGES_DIR, "count.json")

FOLDERS = ["vd", "vl", "hd", "hl"]

scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
)


def load_json(filepath: str, default=None):
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    return default if default is not None else {}


def save_json(filepath: str, data):
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_file_hash(filepath: str) -> str:
    sha256 = hashlib.sha256()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256.update(chunk)
    return sha256.hexdigest()


def load_hash_registry() -> dict:
    registry_path = os.path.join(IMAGES_DIR, "hash_registry.json")
    return load_json(registry_path, {})


def save_hash_registry(registry: dict):
    registry_path = os.path.join(IMAGES_DIR, "hash_registry.json")
    save_json(registry_path, registry)


def get_folder_count(folder: str) -> int:
    folder_path = os.path.join(IMAGES_DIR, folder)
    if not os.path.exists(folder_path):
        return 0
    count = 0
    for f in os.listdir(folder_path):
        if f.endswith('.webp'):
            count += 1
    return count


def get_next_gallery():
    galleries = load_json(GALLERIES_FILE, [])
    progress = load_json(PROGRESS_FILE, {"completed": []})
    completed = set(progress.get("completed", []))
    
    for gallery in galleries:
        if gallery["url"] not in completed:
            return gallery
    return None


def mark_completed(url: str):
    progress = load_json(PROGRESS_FILE, {"completed": []})
    if url not in progress["completed"]:
        progress["completed"].append(url)
    save_json(PROGRESS_FILE, progress)


def scrape_images(url: str) -> list[dict]:
    print(f"ğŸŒ æ­£åœ¨çˆ¬å–: {url}")
    
    try:
        resp = scraper.get(url, timeout=30)
        resp.raise_for_status()
        resp.encoding = 'utf-8'
        print(f"âœ… é¡µé¢è¯·æ±‚æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è¯·æ±‚é¡µé¢å¤±è´¥: {e}")
        return []
    
    soup = BeautifulSoup(resp.text, "lxml")
    images = []
    links = soup.find_all("a", {"data-fancybox": True})
    
    for idx, link in enumerate(links, 1):
        href = link.get("href", "")
        if href and href.startswith("http"):
            images.append({"url": href, "index": idx})
    
    print(f"âœ… æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡")
    return images


def download_image(url: str, save_path: str) -> bool:
    try:
        resp = scraper.get(url, timeout=60, stream=True)
        resp.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in resp.iter_content(chunk_size=8192):
                f.write(chunk)
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


def convert_to_webp(input_path: str, output_path: str) -> bool:
    try:
        img = cv2.imread(input_path)
        if img is None:
            return False
        cv2.imwrite(output_path, img, [cv2.IMWRITE_WEBP_QUALITY, 85])
        return True
    except:
        return False


def get_image_info(path: str, threshold=BRIGHTNESS_THRESHOLD) -> dict | None:
    try:
        img = cv2.imread(path)
        if img is None:
            return None
        
        height, width = img.shape[:2]
        if width < 10 or height < 10:
            return None
        
        orientation = "h" if width >= height else "v"
        
        img_resized = cv2.resize(img, (100, 100))
        lab = cv2.cvtColor(img_resized, cv2.COLOR_BGR2LAB)
        avg_l = lab[:, :, 0].mean()
        brightness = "d" if avg_l < threshold else "l"
        
        folder = orientation + brightness
        
        print(f"ğŸ–¼ï¸ {width}x{height} â†’ L={avg_l:.1f} â†’ {folder}")
        
        return {"folder": folder}
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None


def update_count_file():
    count = {}
    for folder in FOLDERS:
        count[folder] = get_folder_count(folder)
    save_json(COUNT_FILE, count)
    print(f"ğŸ“Š æ›´æ–° count.json: {count}")


def process_gallery(gallery: dict) -> str:
    """
    è¿”å›çŠ¶æ€:
    - "success": æˆåŠŸå¤„ç†
    - "empty": æ²¡æœ‰å›¾ç‰‡ï¼Œéœ€è·³è¿‡
    - "error": å‡ºé”™
    """
    url = gallery["url"]
    folder_name = gallery["folder"]
    
    print(f"\n{'='*50}")
    print(f"ğŸ“‚ å¤„ç†: {folder_name}")
    print(f"{'='*50}\n")
    
    temp_dir = "temp_download"
    os.makedirs(temp_dir, exist_ok=True)
    for folder in FOLDERS:
        os.makedirs(os.path.join(IMAGES_DIR, folder), exist_ok=True)
    
    images = scrape_images(url)
    
    # â­ å…³é”®ä¿®æ”¹ï¼šæ²¡æœ‰å›¾ç‰‡æ—¶è¿”å› "empty"
    if not images:
        print(f"âš ï¸ æ²¡æœ‰å›¾ç‰‡ï¼Œè·³è¿‡æ­¤gallery")
        return "empty"
    
    hash_registry = load_hash_registry()
    folder_counts = {folder: get_folder_count(folder) for folder in FOLDERS}
    
    new_count = 0
    for img_info in images[:BATCH_SIZE]:
        idx = img_info["index"]
        temp_path = os.path.join(temp_dir, f"temp_{idx}")
        
        print(f"\nğŸ“¥ ä¸‹è½½ {idx}/{len(images)}...")
        
        if not download_image(img_info["url"], temp_path):
            continue
        
        file_hash = get_file_hash(temp_path)
        
        if file_hash in hash_registry:
            print(f"â­ï¸ è·³è¿‡é‡å¤: {file_hash[:16]}...")
            os.remove(temp_path)
            continue
        
        info = get_image_info(temp_path)
        if info is None:
            os.remove(temp_path)
            continue
        
        target_folder = info["folder"]
        folder_counts[target_folder] += 1
        new_num = folder_counts[target_folder]
        
        final_path = os.path.join(IMAGES_DIR, target_folder, f"{new_num}.webp")
        
        if convert_to_webp(temp_path, final_path):
            hash_registry[file_hash] = f"{target_folder}/{new_num}.webp"
            new_count += 1
            print(f"âœ… ä¿å­˜: {target_folder}/{new_num}.webp")
        
        os.remove(temp_path)
    
    save_hash_registry(hash_registry)
    update_count_file()
    
    if os.path.exists(temp_dir):
        for f in os.listdir(temp_dir):
            os.remove(os.path.join(temp_dir, f))
        os.rmdir(temp_dir)
    
    print(f"\nâœ… å®Œæˆ: {folder_name}")
    print(f"ğŸ“Š æ–°å¢ {new_count} å¼ ")
    
    return "success"


def main():
    print("ğŸš€ å¼€å§‹è¿è¡Œ")
    
    os.makedirs(IMAGES_DIR, exist_ok=True)
    
    # â­ å…³é”®ä¿®æ”¹ï¼šå¾ªç¯å¤„ç†ï¼Œè·³è¿‡ç©ºgallery
    while True:
        gallery = get_next_gallery()
        if gallery is None:
            print("\nğŸ‰ æ‰€æœ‰å›¾ä»“å·²å¤„ç†å®Œæˆ!")
            break
        
        result = process_gallery(gallery)
        
        if result == "empty":
            # ç©ºgalleryï¼Œæ ‡è®°å®Œæˆå¹¶ç»§ç»­ä¸‹ä¸€ä¸ª
            print(f"â­ï¸ è·³è¿‡ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª...\n")
            mark_completed(gallery["url"])
            continue
        elif result == "success":
            # æˆåŠŸå¤„ç†ï¼Œæ ‡è®°å®Œæˆå¹¶é€€å‡º
            mark_completed(gallery["url"])
            break
        else:
            # å‡ºé”™ï¼Œä¸æ ‡è®°ï¼Œé€€å‡ºç­‰å¾…é‡è¯•
            print(f"âŒ å¤„ç†å‡ºé”™ï¼Œä¸‹æ¬¡é‡è¯•")
            break
    
    print("\nğŸ ç»“æŸ")


if __name__ == "__main__":
    main()
